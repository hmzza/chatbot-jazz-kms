Flask==3.1.0 # Web API and UI rendering
langchain==0.3.23 # Core LLM + RAG framework
langchain-community==0.3.21 # Community integrations (FAISS, Ollama)
langchain-huggingface==0.0.6 # Use HuggingFaceEmbeddings correctly

sentence-transformers==4.1.0 # Embedding generation
huggingface-hub==0.30.2 # Downloads from HuggingFace
faiss-cpu==1.10.0 # Vector similarity search (CPU version)
torch==2.6.0 # Required by sentence-transformers
numpy==2.2.4 # Used by FAISS, Torch
requests==2.32.3 # API calls & model downloads
ollama==0.4.7 # LLM server interface (local llama3)
python-dotenv==1.1.0 # Load config from .env file
tqdm # Nice loading progress bars
orjson # Fast JSON parser (used by LangChain)
pydantic==2.11.4 # Used by LangChain internally
# ----------------------------------
# SETUP INSTRUCTIONS
# ----------------------------------
# 1. Ensure you have Python 3.8+ installed on your system.
# 2. Create a virtual environment (optional but recommended):
#    - On Unix/MacOS: python -m venv venv
#    - On Windows: python -m venv venv
# 3. Activate the virtual environment:
#    - On Unix/MacOS: source venv/bin/activate
#    - On Windows: venv\Scripts\activate
# 4. Install the dependencies:
#    pip install -r requirements.txt
#    - If you encounter version conflicts (e.g., with torch), install manually with specific versions:
#      pip install torch==2.6.0
# 5. Install additional system dependencies (if needed):
#    - For faiss-cpu: Ensure a C++ compiler (e.g., g++) is installed.
#    - For sentence-transformers and torch: Ensure compatible CUDA libraries if using GPU (optional).
# 6. Start the Ollama server (required for the LLM):
#    - Follow Ollama installation instructions at https://ollama.com/docs
#    - Run: ollama serve
# 7. Run the application:
#    python app.py
# 8. Access the chat interface in your browser at http://0.0.0.0:6066
# 9. (Optional) Create a .env file for environment variables if using python-dotenv:
#    - Example: API_KEY=your_api_key
#    - Load it in app.py with: from dotenv import load_dotenv; load_dotenv()

# ----------------------------------
# NOTES
# ----------------------------------
# - The torch version (2.6.0) is CPU-only. For GPU support, adjust the version and install with CUDA:
#   pip install torch==2.6.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html
# - If dependencies fail to install, check for compatibility issues and adjust versions in this file.
# - Ensure the 'cleaned_data' directory exists or is created as per the code logic.